{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9-Z82i495Gl"
   },
   "source": [
    "#       Seq2Seq: Text Summarization with Keras\n",
    "#### ディープラーニングによる文章要約\n",
    "![](http://abigailsee.com/img/pointer-gen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw_oyfrE95Go"
   },
   "source": [
    "## Process\n",
    "1. Preprocessing\n",
    "2. Word2vec\n",
    "3. Building Seq2Seq Architecture\n",
    "4. Training with  BBC article&summary Dataset\n",
    "5. Generate Summary with my_summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2QnkkQv95Gq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeKq5tai95Gu"
   },
   "outputs": [],
   "source": [
    "news_category = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "row_doc = \"/Users/akr712/Desktop/文章要約/Row News Articles/\"\n",
    "summary_doc = \"/Users/akr712/Desktop/文章要約/Summaries/\"\n",
    "\n",
    "data={\"articles\":[], \"summaries\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "682uVCoC95Gy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "directories = {\"news\": row_doc, \"summary\": summary_doc}\n",
    "row_dict = {}\n",
    "sum_dict = {}\n",
    "\n",
    "for path in directories.values():\n",
    "    if path == row_doc:\n",
    "        file_dict = row_dict\n",
    "    else:\n",
    "        file_dict = sum_dict\n",
    "    dire = path\n",
    "    for cat in news_category:\n",
    "        category = cat\n",
    "        files = os.listdir(dire + category)\n",
    "        file_dict[cat] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM75fs9g95Hc"
   },
   "outputs": [],
   "source": [
    "row_data = {}\n",
    "for cat in row_dict.keys():\n",
    "    cat_dict = {}\n",
    "    # row_data_frame[cat] = []\n",
    "    for i in range(0, len(row_dict[cat])):\n",
    "        filename = row_dict[cat][i]\n",
    "        path = row_doc + cat + \"/\" + filename\n",
    "        with open(path, \"rb\") as f:                \n",
    "            text = f.read()\n",
    "            cat_dict[filename[:3]] = text\n",
    "    row_data[cat] = cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjPzSMPT95Hg"
   },
   "outputs": [],
   "source": [
    "sum_data = {}\n",
    "for cat in sum_dict.keys():\n",
    "    cat_dict = {}\n",
    "    # row_data_frame[cat] = []\n",
    "    for i in range(0, len(sum_dict[cat])):\n",
    "        filename = sum_dict[cat][i]\n",
    "        path = summary_doc + cat + \"/\" + filename\n",
    "        with open(path, \"rb\") as f:                \n",
    "            text = f.read()\n",
    "            cat_dict[filename[:3]] = text\n",
    "    sum_data[cat] = cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "du70GJ7o95H0",
    "outputId": "2f05e596-129f-489e-8fcd-e789f870fe9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_business = pd.DataFrame.from_dict(row_data[\"business\"], orient=\"index\", columns=[\"row_article\"])\n",
    "news_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKbrhIYw95H8"
   },
   "outputs": [],
   "source": [
    "#  news_category = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "news_entertainment = pd.DataFrame.from_dict(row_data[\"entertainment\"], orient=\"index\", columns=[\"row_article\"])\n",
    "news_politics = pd.DataFrame.from_dict(row_data[\"politics\"], orient=\"index\", columns=[\"row_article\"])\n",
    "news_sport = pd.DataFrame.from_dict(row_data[\"sport\"], orient=\"index\", columns=[\"row_article\"])\n",
    "news_tech = pd.DataFrame.from_dict(row_data[\"tech\"], orient=\"index\", columns=[\"row_article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KrSmXmON95IE"
   },
   "outputs": [],
   "source": [
    "# summary data\n",
    "summary_business = pd.DataFrame.from_dict(sum_data[\"business\"], orient=\"index\", columns=[\"summary\"])\n",
    "summary_entertainment = pd.DataFrame.from_dict(sum_data[\"entertainment\"], orient=\"index\", columns=[\"summary\"])\n",
    "summary_politics = pd.DataFrame.from_dict(sum_data[\"politics\"], orient=\"index\", columns=[\"summary\"])\n",
    "summary_sport = pd.DataFrame.from_dict(sum_data[\"sport\"], orient=\"index\", columns=[\"summary\"])\n",
    "summary_tech = pd.DataFrame.from_dict(sum_data[\"tech\"], orient=\"index\", columns=[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNL3-2xL95IJ",
    "outputId": "8dd40f58-2921-4c73-fd62-6b5b9e673169"
   },
   "outputs": [],
   "source": [
    "summary_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aq407qkG95IO"
   },
   "outputs": [],
   "source": [
    "business = news_business.join(summary_business, how='inner')\n",
    "entertainment = news_entertainment.join(summary_entertainment, how='inner')\n",
    "politics = news_politics.join(summary_politics, how='inner')\n",
    "sport = news_sport.join(summary_sport, how='inner')\n",
    "tech = news_tech.join(summary_tech, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKbjkn-r95IR"
   },
   "outputs": [],
   "source": [
    "business = news_business.join(summary_business, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKfHtKbL95IW",
    "outputId": "11b33d28-b909-4924-8e97-3e978c473fe8"
   },
   "outputs": [],
   "source": [
    "business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sUHqLUJ95Ib",
    "outputId": "7b492210-e0b1-4622-fcba-9c0f730ca3cd"
   },
   "outputs": [],
   "source": [
    "print(\"row\", len(business.iloc[0,0]))\n",
    "print(\"sum\", len(business.iloc[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VthiJz-h95Ij"
   },
   "outputs": [],
   "source": [
    "list_df = [business, entertainment, politics, sport, tech]\n",
    "length = 0\n",
    "for df in list_df:\n",
    "    length += len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hj44ZFO-95Iu",
    "outputId": "f5649126-2cf2-45ad-cc77-879985450f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of all data:  2225\n"
     ]
    }
   ],
   "source": [
    "print(\"length of all data: \", length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vFtKW1DS95I2",
    "outputId": "f9e46aae-aa16-4415-8302-ddd47bd220ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_df = pd.concat([business, entertainment, politics, sport, tech], ignore_index=True)\n",
    "len(bbc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xogCv95T95JF"
   },
   "source": [
    "## Step 2. Preprocessing Text Data\n",
    "1. Clean Text\n",
    "2. Tokenize\n",
    "3. Vocabrary\n",
    "4. Padding\n",
    "5. One-Hot Encoding\n",
    "6. Reshape to (MAX_LEN, One-Hot Encoding DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y2un_KkA95JU"
   },
   "source": [
    "### 2-1. Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxQY2wvq95JJ"
   },
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    text = str(text)\n",
    "    text=text.split()\n",
    "    words=[]\n",
    "    for t in text:\n",
    "        if t.isalpha():\n",
    "            words.append(t)\n",
    "    text=\" \".join(words)\n",
    "    text=text.lower()\n",
    "    text=re.sub(r\"what's\",\"what is \",text)\n",
    "    text=re.sub(r\"it's\",\"it is \",text)\n",
    "    text=re.sub(r\"\\'ve\",\" have \",text)\n",
    "    text=re.sub(r\"i'm\",\"i am \",text)\n",
    "    text=re.sub(r\"\\'re\",\" are \",text)\n",
    "    text=re.sub(r\"n't\",\" not \",text)\n",
    "    text=re.sub(r\"\\'d\",\" would \",text)\n",
    "    text=re.sub(r\"\\'s\",\"s\",text)\n",
    "    text=re.sub(r\"\\'ll\",\" will \",text)\n",
    "    text=re.sub(r\"can't\",\" cannot \",text)\n",
    "    text=re.sub(r\" e g \",\" eg \",text)\n",
    "    text=re.sub(r\"e-mail\",\"email\",text)\n",
    "    text=re.sub(r\"9\\\\/11\",\" 911 \",text)\n",
    "    text=re.sub(r\" u.s\",\" american \",text)\n",
    "    text=re.sub(r\" u.n\",\" united nations \",text)\n",
    "    text=re.sub(r\"\\n\",\" \",text)\n",
    "    text=re.sub(r\":\",\" \",text)\n",
    "    text=re.sub(r\"-\",\" \",text)\n",
    "    text=re.sub(r\"\\_\",\" \",text)\n",
    "    text=re.sub(r\"\\d+\",\" \",text)\n",
    "    text=re.sub(r\"[$#@%&*!~?%{}()]\",\" \",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7ovvCt495JL"
   },
   "outputs": [],
   "source": [
    "for col in bbc_df.columns:\n",
    "    bbc_df[col] = bbc_df[col].apply(lambda x: cleantext(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwbxBACO95JP",
    "outputId": "8b9cf24a-1714-47bf-944b-4aaa54ab78c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise on new man utd in manchester united close...</td>\n",
       "      <td>united revealed on sunday that it had received...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>confidence dips in confidence among japanese m...</td>\n",
       "      <td>confidence among japanese manufacturers has we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>makes new man utd glazer has made a fresh appr...</td>\n",
       "      <td>glazer has made a fresh approach to buy manche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hope for borussia in struggling german footbal...</td>\n",
       "      <td>in struggling german football club borussia do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>airlines hit tunnel operator eurotunnel has se...</td>\n",
       "      <td>firm said sales were down in to euros the mome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         row_article  \\\n",
       "0  rise on new man utd in manchester united close...   \n",
       "1  confidence dips in confidence among japanese m...   \n",
       "2  makes new man utd glazer has made a fresh appr...   \n",
       "3  hope for borussia in struggling german footbal...   \n",
       "4  airlines hit tunnel operator eurotunnel has se...   \n",
       "\n",
       "                                             summary  \n",
       "0  united revealed on sunday that it had received...  \n",
       "1  confidence among japanese manufacturers has we...  \n",
       "2  glazer has made a fresh approach to buy manche...  \n",
       "3  in struggling german football club borussia do...  \n",
       "4  firm said sales were down in to euros the mome...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "JBPbkd8pIORs",
    "outputId": "51066e27-b9c1-467e-845a-b162e813ce2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>b'Robotic pods take on car design\\n\\nA new bre...</td>\n",
       "      <td>b'Dr Erel Avineri, of the Centre for Transport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>b'More power to the people says HP\\n\\nThe digi...</td>\n",
       "      <td>b'She said the goal for 2005 was to make peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>b\"Disney backs Sony DVD technology\\n\\nA next g...</td>\n",
       "      <td>b\"Film giant Disney says it will produce its f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>b'Gamer buys $26,500 virtual land\\n\\nA 22-year...</td>\n",
       "      <td>b'The land exists within the game Project Entr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>b'Windows worm travels with Tetris\\n\\nUsers ar...</td>\n",
       "      <td>b'The Cellery worm installs a playable version...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           row_article  \\\n",
       "229  b'Robotic pods take on car design\\n\\nA new bre...   \n",
       "295  b'More power to the people says HP\\n\\nThe digi...   \n",
       "219  b\"Disney backs Sony DVD technology\\n\\nA next g...   \n",
       "249  b'Gamer buys $26,500 virtual land\\n\\nA 22-year...   \n",
       "272  b'Windows worm travels with Tetris\\n\\nUsers ar...   \n",
       "\n",
       "                                               summary  \n",
       "229  b'Dr Erel Avineri, of the Centre for Transport...  \n",
       "295  b'She said the goal for 2005 was to make peopl...  \n",
       "219  b\"Film giant Disney says it will produce its f...  \n",
       "249  b'The land exists within the game Project Entr...  \n",
       "272  b'The Cellery worm installs a playable version...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YA9tvTPUIciB",
    "outputId": "1780794c-4748-49ba-8669-c75b875347d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2969"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list =[]\n",
    "for article in df.row_article:\n",
    "    words = article.split()\n",
    "    length = len(words)\n",
    "    len_list.append(length)\n",
    "max(len_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jO14nm8JJPzZ"
   },
   "source": [
    "### 2-2. Tokenizer\n",
    "1. Tokenize and One-Hot : Tokenizer\n",
    "2. Vocabraly: article and summary 15000 words \n",
    "3. Padding: pad_sequences 1000 max_len\n",
    "4. Reshape: manual max_len * one-hot matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_article</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>continues rapid economy has expanded by a brea...</td>\n",
       "      <td>overall investment in fixed assets was still u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deccan seals deccan has ordered airbus planes ...</td>\n",
       "      <td>government has given its backing to cheaper an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>job growth continues in us created fewer jobs ...</td>\n",
       "      <td>creation was one of last main concerns for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owner buys rival for retail giant federated de...</td>\n",
       "      <td>retail giant federated department stores is to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>secures giant japan is to supply japan airline...</td>\n",
       "      <td>chose the after carefully considering both it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         row_article  \\\n",
       "0  continues rapid economy has expanded by a brea...   \n",
       "1  deccan seals deccan has ordered airbus planes ...   \n",
       "2  job growth continues in us created fewer jobs ...   \n",
       "3  owner buys rival for retail giant federated de...   \n",
       "4  secures giant japan is to supply japan airline...   \n",
       "\n",
       "                                             summary  \n",
       "0  overall investment in fixed assets was still u...  \n",
       "1  government has given its backing to cheaper an...  \n",
       "2  creation was one of last main concerns for the...  \n",
       "3  retail giant federated department stores is to...  \n",
       "4  chose the after carefully considering both it ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_art_sum = pd.read_csv(\"cleaned_bbc_news.csv\")\n",
    "bbc_art_sum.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "bbc_art_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list(bbc_art_sum.row_article)\n",
    "summaries = list(bbc_art_sum.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-1. Tokenize: text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W9aALHY_YbUN",
    "outputId": "b228cf03-c36b-413a-d3c8-c9cb6fff1b36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23914"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "VOCAB_SIZE = 14999\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(articles)\n",
    "article_sequences = tokenizer.texts_to_sequences(articles)\n",
    "art_word_index = tokenizer.word_index\n",
    "len(art_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1411, 2338, 248, 16, 3994, 22, 5, 6483, 165, 1359, 50, 966, 4, 120, 967, 176, 118, 505, 38, 2339]\n",
      "[5211, 8881, 5211, 16, 2233, 3001, 3441, 6, 5, 217, 18, 60, 1270, 7874, 6, 1, 827, 5211, 11, 108]\n",
      "[478, 196, 1411, 6, 54, 736, 2283, 498, 50, 164, 6, 24, 349, 17, 9, 1, 3322, 6, 5213, 11]\n"
     ]
    }
   ],
   "source": [
    "print(article_sequences[0][:20])\n",
    "print(article_sequences[1][:20])\n",
    "print(article_sequences[2][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-2. Vocabraly: article and summary 15000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_word_index_1500 = {}\n",
    "counter = 0\n",
    "for word in art_word_index.keys():\n",
    "    if art_word_index[word] == 0:\n",
    "        print(\"found 0!\")\n",
    "        break\n",
    "    if art_word_index[word] > VOCAB_SIZE:\n",
    "        continue\n",
    "    else:\n",
    "        art_word_index_1500[word] = art_word_index[word]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23929"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(summaries)\n",
    "summary_sequences = tokenizer.texts_to_sequences(summaries)\n",
    "sum_word_index = tokenizer.word_index\n",
    "len(sum_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_word_index_1500 = {}\n",
    "counter = 0\n",
    "for word in sum_word_index.keys():\n",
    "    if sum_word_index[word] == 0:\n",
    "        print(\"found 0!\")\n",
    "        break\n",
    "    if sum_word_index[word] > VOCAB_SIZE:\n",
    "        continue\n",
    "    else:\n",
    "        sum_word_index_1500[word] = sum_word_index[word]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-3. Padding: pad_sequences 1000 max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_LEN = 1000\n",
    "pad_art_sequences = pad_sequences(article_sequences, maxlen=MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(article_sequences[1]), len(pad_art_sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sum_sequences = pad_sequences(summary_sequences, maxlen=MAX_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(summary_sequences[1]), len(pad_sum_sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_art_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1411, 2338,  248, ...,    0,    0,    0],\n",
       "       [5211, 8881, 5211, ...,    0,    0,    0],\n",
       "       [ 478,  196, 1411, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 421, 1337, 2012, ...,    0,    0,    0],\n",
       "       [2164,  267, 1109, ...,    0,    0,    0],\n",
       "       [   7,  284,    8, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_art_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-4. Reshape: manual max_len * one-hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使わない\n",
    "\"\"\"\n",
    "encoder_inputs = np.zeros((2225, 1000), dtype='float32')\n",
    "encoder_inputs.shape\n",
    "\n",
    "decoder_inputs = np.zeros((2225, 1000), dtype='float32')\n",
    "decoder_inputs.shape\n",
    "\n",
    "for i, seqs in enumerate(pad_art_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        encoder_inputs[i, j] = seq\n",
    "        \n",
    "for i, seqs in enumerate(pad_sum_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        decoder_inputs[i, j] = seq\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 1000, 15000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs = np.zeros((2225, 1000, 15000), dtype='float32')\n",
    "decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seqs in enumerate(pad_sum_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        decoder_outputs[i, j, seq] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 1000, 15000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2-5. Pre-trained word2vec and word2vec Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sokvMj0Xfnjw",
    "outputId": "3ef40349-e3e7-4f25-d263-7d316d21f829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.200d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdx3r2rvZAwN"
   },
   "outputs": [],
   "source": [
    "def embedding_matrix_creater(embedding_dimention, word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimention))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VB6UmXeqWleO",
    "outputId": "59e76cc0-91a2-454b-a61e-f7cb1b2cf59a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_embedding_matrix = embedding_matrix_creater(200, word_index=art_word_index_1500)\n",
    "art_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1KgrdRFvb7A1",
    "outputId": "e01036f5-476b-46eb-bec7-1ad263ba9dd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embedding_matrix = embedding_matrix_creater(200, word_index=sum_word_index_1500)\n",
    "sum_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuawT3rXUIBY"
   },
   "outputs": [],
   "source": [
    "encoder_embedding_layer = Embedding(input_dim = 15000, \n",
    "                                    output_dim = 200,\n",
    "                                    input_length = MAX_LEN,\n",
    "                                    weights = [art_embedding_matrix],\n",
    "                                    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf3EzUZcIccZ"
   },
   "outputs": [],
   "source": [
    "decoder_embedding_layer = Embedding(input_dim = 15000, \n",
    "                                    output_dim = 200,\n",
    "                                    input_length = MAX_LEN,\n",
    "                                    weights = [sum_embedding_matrix],\n",
    "                                    trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J5acWwCxIcZz",
    "outputId": "a92cd279-b2de-4110-fd1d-35e13e90c099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AB663AaY0PKH"
   },
   "source": [
    "## Step 3. Building Encoder-Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-kZT0pQRqRl"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pydot\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import backend as k\n",
    "k.set_learning_phase(1)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,LSTM,Dropout,Input,Activation,Add,concatenate, Embedding, RepeatVector\n",
    "from keras.layers.advanced_activations import LeakyReLU,PReLU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psWPxabWIcWz"
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "\n",
    "MAX_LEN = 1000\n",
    "VOCAB_SIZE =15000\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_UNITS = 200\n",
    "VOCAB_SIZE = VOCAB_SIZE + 1\n",
    "\n",
    "LEARNING_RATE = 0.002\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. Simple LSTM Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGjWMUG3IcT_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 200)    3000000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1000, 200)    3000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 200)          320800      embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 200)          320800      embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400)          0           lstm_4[0][0]                     \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 15002)        6015802     concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 12,657,402\n",
      "Trainable params: 6,657,402\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple LSTM Encoder-Decoder-seq2seq\n",
    "\"\"\"\n",
    "# encoder\n",
    "encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_UNITS)(encoder_embedding)\n",
    "# decoder\n",
    "decoder_inputs = Input(shape=(MAX_LEN, ))\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(200)(decoder_embedding)\n",
    "# merge\n",
    "merge_layer = concatenate([encoder_LSTM, decoder_LSTM])\n",
    "decoder_outputs = Dense(units=VOCAB_SIZE+1, activation=\"softmax\")(merge_layer) # SUM_VOCAB_SIZE, sum_embedding_matrix.shape[1]\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHw0flwp4uW2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2. Bidirectional LSTM Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bidirectional LSTM: Others Inspired Encoder-Decoder-seq2seq\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(MAX_LEN,))\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
    "encoder_LSTM_R = LSTM(HIDDEN_UNITS, return_state=True, go_backwards=True)\n",
    "encoder_outputs_R, state_h_R, state_c_R = encoder_LSTM_R(encoder_embedding)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "final_h = Add()([state_h, state_h_R])\n",
    "final_c = Add()([state_c, state_c_R])\n",
    "encoder_states = [final_h, final_c]\n",
    "\n",
    "\"\"\"\n",
    "decoder\n",
    "\"\"\"\n",
    "decoder_inputs = Input(shape=(MAX_LEN,))\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states) \n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='linear')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model= Model(inputs=[encoder_inputs,decoder_inputs], outputs=decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3. Chatbot Inspired Encoder-Decoder-seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chatbot Inspired Encoder-Decoder-seq2seq\n",
    "\"\"\"\n",
    "encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "encoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
    "\n",
    "decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_LSTM = LSTM(HIDDEN_UNITS, return_state=True, return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n",
    "outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = RMSprop(lr=0.01, clipnorm=1.)\n",
    "model.compile(loss='mse', optimizer=rmsprop, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 200)    3000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200), (None, 320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 200), (None, 320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1000, 200)    3000000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 200)          0           lstm_1[0][1]                     \n",
      "                                                                 lstm_2[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 200)          0           lstm_1[0][2]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, 1000, 200),  320800      embedding_2[0][0]                \n",
      "                                                                 add_1[0][0]                      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000, 15001)  3015201     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 9,977,601\n",
      "Trainable params: 3,977,601\n",
      "Non-trainable params: 6,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 2\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Training your model and Validate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_samples = len(pad_sum_sequences)\n",
    "decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputの３Dテンソル\n",
    "for i, seqs in enumerate(pad_sum_sequences):\n",
    "    for j, seq in enumerate(seqs):\n",
    "        if j > 0:\n",
    "            decoder_output_data[i][j][seq] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EFxSEKvfIoGf"
   },
   "outputs": [],
   "source": [
    "art_train, art_test, sum_train, sum_test = train_test_split(pad_art_sequences, pad_sum_sequences, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9hn_1hm-JCa2",
    "outputId": "95c79153-06ec-4b2d-b25e-6f34c933ae44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1780"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num = art_train.shape[0]\n",
    "train_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = decoder_output_data[:train_num]\n",
    "target_test = decoder_output_data[train_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1780 samples, validate on 445 samples\n",
      "Epoch 1/5\n",
      " 896/1780 [==============>...............] - ETA: 1:46:08 - loss: 4.1167e-04 - acc: 0.8187"
     ]
    }
   ],
   "source": [
    "history = model.fit([art_train, sum_train], \n",
    "                     target_train, \n",
    "                     epochs=EPOCHS, \n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     validation_data=([art_test, sum_test], target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正確性の可視化\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の可視化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "with open('text_summary.json',\"w\").write(model.to_json())\n",
    "\n",
    "# 重みの読み込み\n",
    "model.load_weights('text_summary.h5')\n",
    "print(\"Saved Model!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "input_matrix.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
